{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read text from csv as dataframe\n",
    "import pandas\n",
    "petitions = pandas.read_csv('C:\\\\Users\\\\Olha\\\\Desktop\\\\TARTU\\\\_Hackathons\\\\Int20h\\\\petitions_data_with_text.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'author', 'start_date', 'status', 'number_of_votes',\n",
       "       'number_of_necessary_votes', 'end_date', 'name', 'url', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#names of columns\n",
    "petitions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 'Повзик Ігор Володимирович', '2015-10-04 12:37:27',\n",
       "       'Підписання завершено (недостатньо підписів)', 494, 10000,\n",
       "       '2016-01-31 20:06:43',\n",
       "       'Повністю реконструювати Шулявську розв&#039;язку',\n",
       "       'https://petition.kievcity.gov.ua/petition/?pid=3',\n",
       "       'Це питання не вирішується роками.\\r\\n Розв&rsquo;язка дуже незручна для водіїв. Міст скоро впаде на голови пішоходів. Процвітає стихійна торгівля. \\r\\n\\r\\n p.s. (як приклад проект Віктора Петрука) '], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of one row - one petition\n",
    "petitions.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4536\n"
     ]
    }
   ],
   "source": [
    "#number of petitions\n",
    "print(len(petitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successful petitions: 46\n",
      "Number of not successful petitions: 4490\n",
      "4536\n"
     ]
    }
   ],
   "source": [
    "#number of non-successful/successful petitions\n",
    "non_success_ids = []\n",
    "success_ids = []\n",
    "for i in range(len(petitions)):\n",
    "    if '(недостатньо підписів)' in petitions['status'].values[i]:\n",
    "        non_success_ids.append(i)\n",
    "    elif '(достатньо підписів)' in petitions['status'].values[i]:\n",
    "        success_ids.append(i)\n",
    "    elif \"На підписанні\" in petitions['status'].values[i]:\n",
    "        if petitions['number_of_votes'].values[i] >= petitions['number_of_necessary_votes'].values[i]/2: \n",
    "            success_ids.append(i)\n",
    "        else: \n",
    "            non_success_ids.append(i)\n",
    "    else:\n",
    "        print('Unusual status:', petitions['status'].values[i])\n",
    "print(\"Number of successful petitions:\", len(success_ids))\n",
    "print(\"Number of not successful petitions:\", len(non_success_ids))\n",
    "print(len(non_success_ids) + len(success_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percenteges for successful petitions\n",
    "\n",
    "success_perc = {}\n",
    "for i in success_ids:\n",
    "    success_perc[i] = round(petitions['number_of_votes'].values[i]/petitions['number_of_necessary_votes'].values[i], 4)\n",
    "#print(sorted(success_perc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "general_good = []\n",
    "for i in success_ids:\n",
    "    general_good.append(petitions['text'].values[i])\n",
    "    \n",
    "general_bad = []\n",
    "for i in non_success_ids:\n",
    "    general_bad.append(petitions['text'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].replace('&rsquo;', \"'\")\n",
    "        text[i] = text[i].replace(\"\\'\", \"'\")\n",
    "        text[i] = text[i].replace('&#039;', \"'\")\n",
    "        text[i] = text[i].replace('\\n', '')\n",
    "        text[i] = text[i].replace('\\n&bull;', '')\n",
    "        text[i] = text[i].replace('\\r', '')\n",
    "        text[i] = text[i].replace('&amp;', '&')\n",
    "        text[i] = text[i].replace('&laquo;', '\"')\n",
    "        text[i] = text[i].replace('&copy;', '')\n",
    "        text[i] = text[i].replace('&mdash;', '-')    \n",
    "        text[i] = text[i].replace('&ndash;', '-')\n",
    "        text[i] = text[i].replace('&raquo;', '\"')\n",
    "clean(general_good)\n",
    "clean(general_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import numpy\n",
    "import sys\n",
    "import codecs\n",
    "import operator\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In successful petitions:\n",
      "Number of words:  8956\n",
      "Number of sentences:  318\n",
      "Number of unique words:  4092\n",
      "The average number of words in sentences:  32.0\n",
      "The average lenght of words:  6.0\n",
      "In non successful petitions:\n",
      "Number of words:  512168\n",
      "Number of sentences:  22746\n",
      "Number of unique words:  77523\n",
      "The average number of words in sentences:  26.0\n",
      "The average lenght of words:  5.0\n"
     ]
    }
   ],
   "source": [
    "def numeric(text):\n",
    "    #sentences, words and word types\n",
    "    words = []\n",
    "    sents = []\n",
    "    for i in range(len(text)):\n",
    "        words += word_tokenize(text[i])\n",
    "        sents += sent_tokenize(text[i])\n",
    "\n",
    "    nwords = len(words)\n",
    "    nsents = len(sents)\n",
    "    nwtypes = len(list(set(words)))\n",
    "\n",
    "    print(\"Number of words: \", nwords)\n",
    "    print(\"Number of sentences: \", nsents)\n",
    "    print(\"Number of unique words: \", nwtypes)\n",
    "\n",
    "    #The average length of the sentences\n",
    "    lens = []\n",
    "    for i in range (0, nsents):\n",
    "        lens.append(len(sents[i]))\n",
    "    avg_sent_length = numpy.mean(lens)\n",
    "\n",
    "    #The average length of the words\n",
    "    distinct_word_lengths = []    \n",
    "    for i in range (0, nwords):\n",
    "        distinct_word_lengths.append(len(words[i]))\n",
    "    avg_word_length = numpy.mean(distinct_word_lengths)\n",
    "    \n",
    "    avg_sents = round(avg_sent_length/avg_word_length)\n",
    "    avg_words = round(avg_word_length)\n",
    "\n",
    "    print(\"The average number of words in sentences: \", avg_sents)\n",
    "    print(\"The average lenght of words: \", avg_words)\n",
    "    \n",
    "    return words, nwords, nsents, nwtypes, lens, avg_sents, distinct_word_lengths, avg_words\n",
    "\n",
    "print(\"In successful petitions:\")\n",
    "words_good, nwords_good, nsents_good, nwtypes_good, lens_good, avg_sents_good, distinct_word_lengths_good, avg_words_good = numeric(general_good)\n",
    "print(\"In non successful petitions:\")\n",
    "words_bad, nwords_bad, nsents_bad, nwtypes_bad, lens_bad, avg_sents_bad, distinct_word_lengths_good, avg_words_bad = numeric(general_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=5.170691673606739, pvalue=2.3517623150655901e-07)\n"
     ]
    }
   ],
   "source": [
    "#statistic hypotesis\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "a = lens_good\n",
    "b = lens_bad\n",
    "\n",
    "print(ttest_ind(a, b))\n",
    "#result: good petitions have bigger lenght of sentences than bad ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequency(words):\n",
    "    d = dict()\n",
    "    for w in words:\n",
    "        if w in d.keys():\n",
    "            d[w] += 1\n",
    "        else:\n",
    "            d[w] = 1\n",
    "    sorted_d = sorted(d.items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "    #for k in sorted_d:\n",
    "        #print k[0], k[1]\n",
    "\n",
    "    ###With filtering\n",
    "    sign = [',', '.', '-', '!', '?', ':', ';', '...', '''''' , '``', '..', '*', '—', '\\'\\'', '’']\n",
    "    cleaned_words = [word for word in words if word not in sign]\n",
    "    d_filtered = dict()\n",
    "    for w in cleaned_words:\n",
    "        if w in d_filtered.keys():\n",
    "            d_filtered[w] += 1\n",
    "        else:\n",
    "            d_filtered[w] = 1\n",
    "    cleaned_words = sorted(d_filtered.items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "    for k in cleaned_words[:50]:\n",
    "        print k[0], k[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
